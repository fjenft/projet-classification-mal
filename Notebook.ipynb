{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 48px; color: red;\">Classification à partir de la base de données My Anime List</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px; color: green;\">Partie I : Récupération des données à partir de l'API de MyAnimeList puis nettoyage des données</h1>\n",
    "\n",
    "Cliquez [ici](https://myanimelist.net/apiconfig/references/api/v2) pour accéder au site de l'API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 22px; color: blue;\">1/Récupération des données à partir de l'API de MyAnimeList</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 collected for the moment...\n",
      "200 collected for the moment...\n",
      "300 collected for the moment...\n",
      "400 collected for the moment...\n",
      "500 collected for the moment...\n",
      "600 collected for the moment...\n",
      "700 collected for the moment...\n",
      "800 collected for the moment...\n",
      "900 collected for the moment...\n",
      "1000 collected for the moment...\n",
      "1100 collected for the moment...\n",
      "1200 collected for the moment...\n",
      "1300 collected for the moment...\n",
      "1400 collected for the moment...\n",
      "1500 collected for the moment...\n",
      "1600 collected for the moment...\n",
      "1700 collected for the moment...\n",
      "1800 collected for the moment...\n",
      "1900 collected for the moment...\n",
      "2000 collected for the moment...\n",
      "2100 collected for the moment...\n",
      "2200 collected for the moment...\n",
      "2300 collected for the moment...\n",
      "2400 collected for the moment...\n",
      "2500 collected for the moment...\n",
      "2600 collected for the moment...\n",
      "2700 collected for the moment...\n",
      "2800 collected for the moment...\n",
      "2900 collected for the moment...\n",
      "3000 collected for the moment...\n",
      "3100 collected for the moment...\n",
      "3200 collected for the moment...\n",
      "3300 collected for the moment...\n",
      "3400 collected for the moment...\n",
      "3500 collected for the moment...\n",
      "3600 collected for the moment...\n",
      "3700 collected for the moment...\n",
      "3800 collected for the moment...\n",
      "3900 collected for the moment...\n",
      "4000 collected for the moment...\n",
      "4100 collected for the moment...\n",
      "4200 collected for the moment...\n",
      "4300 collected for the moment...\n",
      "4400 collected for the moment...\n",
      "4500 collected for the moment...\n",
      "4600 collected for the moment...\n",
      "4700 collected for the moment...\n",
      "4800 collected for the moment...\n",
      "4900 collected for the moment...\n",
      "5000 collected for the moment...\n",
      "5100 collected for the moment...\n",
      "5200 collected for the moment...\n",
      "5300 collected for the moment...\n",
      "5400 collected for the moment...\n",
      "5500 collected for the moment...\n",
      "5600 collected for the moment...\n",
      "5700 collected for the moment...\n",
      "5800 collected for the moment...\n",
      "5900 collected for the moment...\n",
      "6000 collected for the moment...\n",
      "6100 collected for the moment...\n",
      "6200 collected for the moment...\n",
      "6300 collected for the moment...\n",
      "6400 collected for the moment...\n",
      "6500 collected for the moment...\n",
      "6600 collected for the moment...\n",
      "6700 collected for the moment...\n",
      "6800 collected for the moment...\n",
      "6900 collected for the moment...\n",
      "7000 collected for the moment...\n",
      "7100 collected for the moment...\n",
      "7200 collected for the moment...\n",
      "7300 collected for the moment...\n",
      "7400 collected for the moment...\n",
      "7500 collected for the moment...\n",
      "7600 collected for the moment...\n",
      "7700 collected for the moment...\n",
      "7800 collected for the moment...\n",
      "7900 collected for the moment...\n",
      "8000 collected for the moment...\n",
      "8100 collected for the moment...\n",
      "8200 collected for the moment...\n",
      "8300 collected for the moment...\n",
      "8400 collected for the moment...\n",
      "8500 collected for the moment...\n",
      "8600 collected for the moment...\n",
      "8700 collected for the moment...\n",
      "8800 collected for the moment...\n",
      "8900 collected for the moment...\n",
      "9000 collected for the moment...\n",
      "9100 collected for the moment...\n",
      "9200 collected for the moment...\n",
      "9300 collected for the moment...\n",
      "9400 collected for the moment...\n",
      "9500 collected for the moment...\n",
      "9600 collected for the moment...\n",
      "9700 collected for the moment...\n",
      "9800 collected for the moment...\n",
      "9900 collected for the moment...\n",
      "10000 collected for the moment...\n",
      "10100 collected for the moment...\n",
      "10200 collected for the moment...\n",
      "10300 collected for the moment...\n",
      "10400 collected for the moment...\n",
      "10500 collected for the moment...\n",
      "10600 collected for the moment...\n",
      "10700 collected for the moment...\n",
      "10800 collected for the moment...\n",
      "10900 collected for the moment...\n",
      "11000 collected for the moment...\n",
      "11100 collected for the moment...\n",
      "11200 collected for the moment...\n",
      "11300 collected for the moment...\n",
      "11400 collected for the moment...\n",
      "11500 collected for the moment...\n",
      "11600 collected for the moment...\n",
      "11700 collected for the moment...\n",
      "11800 collected for the moment...\n",
      "11900 collected for the moment...\n",
      "12000 collected for the moment...\n",
      "12100 collected for the moment...\n",
      "12200 collected for the moment...\n",
      "12300 collected for the moment...\n",
      "12400 collected for the moment...\n",
      "12500 collected for the moment...\n",
      "12600 collected for the moment...\n",
      "12700 collected for the moment...\n",
      "12800 collected for the moment...\n",
      "12900 collected for the moment...\n",
      "13000 collected for the moment...\n",
      "13100 collected for the moment...\n",
      "13200 collected for the moment...\n",
      "13300 collected for the moment...\n",
      "13400 collected for the moment...\n",
      "13500 collected for the moment...\n",
      "13600 collected for the moment...\n",
      "13700 collected for the moment...\n",
      "13800 collected for the moment...\n",
      "13900 collected for the moment...\n",
      "14000 collected for the moment...\n",
      "14100 collected for the moment...\n",
      "14200 collected for the moment...\n",
      "14300 collected for the moment...\n",
      "14400 collected for the moment...\n",
      "14500 collected for the moment...\n",
      "14600 collected for the moment...\n",
      "14700 collected for the moment...\n",
      "14800 collected for the moment...\n",
      "14900 collected for the moment...\n",
      "15000 collected for the moment...\n",
      "15100 collected for the moment...\n",
      "15200 collected for the moment...\n",
      "15300 collected for the moment...\n",
      "15400 collected for the moment...\n",
      "15500 collected for the moment...\n",
      "15600 collected for the moment...\n",
      "15700 collected for the moment...\n",
      "15800 collected for the moment...\n",
      "15900 collected for the moment...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "#Comme l'API ne nous permet pas de récupérer les animes en groupe à travers leur ID, on récupère la liste des animés et leur caractéristiques à partir de leur le rang, dans l'ordre décroissant décroissant (en prenant les 100 1ers rangs, puis les 100 rangs suivants ainsi de suite...)\n",
    "\n",
    "all_anime = [] \n",
    "nbr_needed = 27490  #Total number of anime on MAL as of 13/11/2024 (obtained by looking directly on the site of myanimelist)\n",
    "\n",
    "ID = {'X-MAL-CLIENT-ID': 'c2db532c391bf31339ffd6afa650d528'} #id client obtenu après s'être inscrit sur My Anime List et avoir fait une demande\n",
    "url = 'https://api.myanimelist.net/v2/anime/ranking'\n",
    "parameters = {\n",
    "    'ranking_type': 'all',  \n",
    "    'limit': 100,  # Max limit per request, divides the total number of anime on mal\n",
    "    'fields': 'id,title,mean,start_date,end_date,rank,popularity,num_list_users,num_scoring_users,nsfw,media_type,status,num_episodes,start_season,broadcast,source,average_episode_duration,rating'\n",
    "}\n",
    "\n",
    "k = 0  # offset but also the number of times the loop is used that is 27490/100 here\n",
    "\n",
    "# Loop until we've collected the target number of anime\n",
    "while k < nbr_needed:\n",
    "    parameters['offset'] = k\n",
    "    mal = requests.get(url, headers=ID, params=parameters)\n",
    "\n",
    "    \n",
    "    if mal.status_code == 200: # Check if the request is successful\n",
    "        data = mal.json()\n",
    "        all_anime.extend(data['data'])\n",
    "        k += parameters['limit']\n",
    "\n",
    "        print(str(len(all_anime)) + \" collected for the moment...\")\n",
    "    \n",
    "        if len(all_anime) >= nbr_needed:\n",
    "            print(\"the total number of anime collected is \" + str(len(all_anime)))\n",
    "            break\n",
    "    else :\n",
    "        print(\"cannot retrieve more than \" + str(len(all_anime))) \n",
    "        break\n",
    "\n",
    "anime_data = pd.DataFrame(all_anime) \n",
    "print(anime_data.head(2))\n",
    "\n",
    "print(anime_data.head())\n",
    "print(anime_data.info())\n",
    "\n",
    "#On voit que le dataframe est constitué du rang des animé et d'un \"node\", un dictionnaire qui contient toutes les caractéristiques de chaque anime.\n",
    "#Il faut donc extraire chaque élément du dictionnaire node pour en faire des colonnes à part entière\n",
    "\n",
    "# On extrait toutes les clés du dictionnaire 'node' et on les transforme en colonnes du dataframe\n",
    "anime_data['node'] = anime_data['node'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)    \n",
    "keys = set().union(*(d.keys() for d in anime_data['node'] if isinstance(d, dict)))\n",
    "for y in keys:\n",
    "    anime_data[f'{y}'] = anime_data['node'].apply(lambda x: x.get(y) if isinstance(x, dict) else None)\n",
    "\n",
    "# On convertit les colonnes contenant des dictionnaires en chaînes\n",
    "for column in anime_data.columns:\n",
    "    if anime_data[column].map(type).eq(dict).any():\n",
    "        anime_data[column] = anime_data[column].apply(lambda x: str(x) if isinstance(x, dict) else x)\n",
    "\n",
    "print(anime_data.head())\n",
    "\n",
    "#On supprime la colonne node qui n'apporte plus d'info\n",
    "anime_data = anime_data.drop(columns=['node'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 22px; color: blue;\">2/Nettoyage des données et enregistrement du dataframe en fichier csv</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anime_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#On vérifie s'il y a des doublons\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nbr_doublons \u001b[38;5;241m=\u001b[39m \u001b[43manime_data\u001b[49m\u001b[38;5;241m.\u001b[39mduplicated()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIl y a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnbr_doublons\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doublons\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#On supprime les colonnes qui ne serviront pas pour la recommendation\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anime_data' is not defined"
     ]
    }
   ],
   "source": [
    "#On vérifie s'il y a des doublons\n",
    "nbr_doublons = anime_data.duplicated().sum()\n",
    "print(f\"Il y a {nbr_doublons} doublons\")\n",
    "\n",
    "#On supprime les colonnes qui ne serviront pas pour la recommendation\n",
    "anime_data=anime_data.drop(columns=['main_picture','broadcast','start_season','end_date'],axis=1)\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(anime_data.head())\n",
    "\n",
    "#On regarde combien de valeurs NaN il y a dans chaque colonne\n",
    "for i in anime_data.columns:\n",
    "    k = anime_data[i].isna().sum()\n",
    "    print(f\"Le nombre de NaN dans la colonne '{i}' est : {k}\")\n",
    "\n",
    "#On gère les différents types de NaN\n",
    "anime_data['source'] = anime_data['source'].fillna('source_inconnue')\n",
    "anime_data['rating'] = anime_data['source'].fillna('rating_inconnu')\n",
    "anime_data['mean'] = anime_data['mean'].fillna(0)\n",
    "anime_data = anime_data.dropna(subset=['rank'])\n",
    "\n",
    "#On veut uniquement garder l'année dans la colonne start_date\n",
    "anime_data['start_date'] = pd.to_datetime(anime_data['start_date'], errors='coerce')  \n",
    "anime_data['start_year'] = anime_data['start_date'].dt.year  \n",
    "anime_data=anime_data.drop(columns=['start_date'],axis=1)\n",
    "\n",
    "anime_data = anime_data.dropna(subset=['start_year'])\n",
    "\n",
    "#On vérifie qu'il n'y a plus de NaN\n",
    "nbr_nan = anime_data.isna().sum().sum()\n",
    "print(f\"Il reste {nbr_nan} NaN\")\n",
    "\n",
    "#On ne garde que les colonnes numériques pour calculer la matrice de correlation\n",
    "anime_data_num = anime_data.select_dtypes(include=[\"number\"])\n",
    "# Calcul de la matrice de corrélation\n",
    "print(anime_data_num.corr())\n",
    "\n",
    "#On sauvegarde le DataFrame en fichier CSV local\n",
    "local_file_path = \"anime_data.csv\"\n",
    "anime_data.to_csv(local_file_path, index=False)\n",
    "print(f\"Fichier CSV sauvegardé localement : {local_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 32px; color: green;\">Partie II : Etudes des données et quelques visualisations</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 22px; color: blue;\">1/Familiarisation avec les données</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On cherche à voir à quoi ressemble les données obtenues sur les animés\n",
    "print(anime_data.head())\n",
    "print(anime_data.describe())\n",
    "print(anime_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On regarde la matrice de corrélation\n",
    "\n",
    "numerical_features = ['num_list_users', 'num_episodes', 'mean', 'rank', 'popularity', 'num_scoring_users', 'start_year']\n",
    "plt.figure(figsize=(12, 8))\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Matrice de correlation')\n",
    "plt.show()\n",
    "plt.savefig('matrice_de_co.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 22px; color: blue;\">2/Etude approfondie des données</h1>\n",
    "\n",
    "On cherche ici à tester et visualiser quelques intuitions qu'on pourrait avoir vis-à-vis des données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
